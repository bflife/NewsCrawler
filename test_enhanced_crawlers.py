#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºç‰ˆæ–°é—»çˆ¬è™«
"""
import sys
import asyncio
from pathlib import Path

sys.path.insert(0, '/home/user/webapp')

from news_crawler.sites.enhanced_crawlers import ENHANCED_CRAWLERS


def test_crawler(crawler_name: str, test_url: str):
    """æµ‹è¯•å•ä¸ªçˆ¬è™«"""
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•çˆ¬è™«: {crawler_name}")
    print(f"æµ‹è¯•URL: {test_url}")
    print(f"{'='*80}\n")
    
    try:
        # è·å–çˆ¬è™«ç±»
        crawler_class = ENHANCED_CRAWLERS.get(crawler_name)
        if not crawler_class:
            print(f"âŒ çˆ¬è™« {crawler_name} ä¸å­˜åœ¨")
            return False
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = crawler_class(
            new_url=test_url,
            save_path="data/test/"
        )
        
        # è¿è¡Œçˆ¬è™«
        print("ğŸ”„ å¼€å§‹æŠ“å–...")
        news_item = crawler.run(persist=True)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nâœ… æŠ“å–æˆåŠŸ!")
        print(f"æ ‡é¢˜: {news_item.title}")
        print(f"å‰¯æ ‡é¢˜: {news_item.subtitle or 'N/A'}")
        print(f"ä½œè€…: {news_item.meta_info.author_name or 'N/A'}")
        print(f"å‘å¸ƒæ—¶é—´: {news_item.meta_info.publish_time or 'N/A'}")
        print(f"å†…å®¹å—æ•°é‡: {len(news_item.contents)}")
        print(f"æ–‡æœ¬æ®µè½: {len(news_item.texts)}")
        print(f"å›¾ç‰‡æ•°é‡: {len(news_item.images)}")
        print(f"è§†é¢‘æ•°é‡: {len(news_item.videos)}")
        print(f"ä¿å­˜è·¯å¾„: {crawler.get_save_json_path()}")
        
        # æ˜¾ç¤ºå‰3æ®µæ–‡æœ¬
        if news_item.texts:
            print(f"\nå‰3æ®µå†…å®¹é¢„è§ˆ:")
            for i, text in enumerate(news_item.texts[:3], 1):
                preview = text[:100] + "..." if len(text) > 100 else text
                print(f"  {i}. {preview}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¢å¼ºç‰ˆæ–°é—»çˆ¬è™«æµ‹è¯•")
    print(f"å…±æœ‰ {len(ENHANCED_CRAWLERS)} ä¸ªå¢å¼ºç‰ˆçˆ¬è™«å¯æµ‹è¯•")
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # ç¾å›½
        # ('cnn', 'https://www.cnn.com/2024/01/01/world/example-article/index.html'),
        # ('nytimes_chinese', 'https://cn.nytimes.com/china/20240101/example-article/'),
        
        # è‹±å›½
        # ('bbc_chinese', 'https://www.bbc.com/zhongwen/simp/chinese-news-12345678'),
        
        # æ—¥æœ¬  
        # ('nhk_news', 'http://www3.nhk.or.jp/news/html/20240101/k10012345671000.html'),
        
        # ä¸­æ–‡
        # ('zaobao', 'https://www.zaobao.com.sg/news/china/story20240101-1234567'),
        # ('hk01', 'https://www.hk01.com/article/123456'),
        # ('ltn', 'https://news.ltn.com.tw/news/politics/breakingnews/1234567'),
    ]
    
    if not test_cases:
        print("\nâš ï¸  æ²¡æœ‰é…ç½®æµ‹è¯•ç”¨ä¾‹")
        print("è¯·åœ¨ä»£ç ä¸­æ·»åŠ çœŸå®çš„æ–°é—»URLè¿›è¡Œæµ‹è¯•")
        print("\nç¤ºä¾‹:")
        print("test_cases = [")
        print("    ('bbc_chinese', 'https://www.bbc.com/zhongwen/simp/chinese-news-12345678'),")
        print("    ('zaobao', 'https://www.zaobao.com.sg/news/china/story20240101-1234567'),")
        print("]")
        return
    
    # è¿è¡Œæµ‹è¯•
    success_count = 0
    fail_count = 0
    
    for crawler_name, test_url in test_cases:
        result = test_crawler(crawler_name, test_url)
        if result:
            success_count += 1
        else:
            fail_count += 1
    
    # æ±‡æ€»
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•å®Œæˆ")
    print(f"{'='*80}")
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"âŒ å¤±è´¥: {fail_count}")
    print(f"æ€»è®¡: {success_count + fail_count}")


if __name__ == "__main__":
    main()
