"""
å¿«é€ŸåŠŸèƒ½æ¼”ç¤ºæµ‹è¯•
éªŒè¯çˆ¬è™«ç³»ç»Ÿçš„å®é™…å·¥ä½œèƒ½åŠ›
"""
import sys
import asyncio
from news_crawler.sites.enhanced_crawlers import ENHANCED_CRAWLERS
from news_crawler.sites.all_crawlers import get_all_crawlers
from news_crawler.scheduler import NewsScheduler

def test_crawler_instantiation():
    """æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ–"""
    print("\n" + "="*80)
    print("æµ‹è¯•1: çˆ¬è™«å®ä¾‹åŒ–")
    print("="*80)
    
    # æµ‹è¯•å¢å¼ºç‰ˆçˆ¬è™«
    print("\nğŸ“± å¢å¼ºç‰ˆçˆ¬è™«:")
    for name, crawler_class in list(ENHANCED_CRAWLERS.items())[:5]:
        try:
            crawler = crawler_class()
            print(f"  âœ… {name}: {crawler.__class__.__name__}")
        except Exception as e:
            print(f"  âŒ {name}: {e}")
    
    # æµ‹è¯•åŸºç¡€çˆ¬è™«
    print("\nğŸ“° åŸºç¡€çˆ¬è™«:")
    all_crawlers = get_all_crawlers()
    for crawler_class in all_crawlers[:5]:
        try:
            crawler = crawler_class()
            name = getattr(crawler, 'name', crawler.__class__.__name__)
            print(f"  âœ… {name}: {crawler.__class__.__name__}")
        except Exception as e:
            print(f"  âŒ {crawler_class}: {e}")

def test_scheduler():
    """æµ‹è¯•è°ƒåº¦å™¨"""
    print("\n" + "="*80)
    print("æµ‹è¯•2: è°ƒåº¦å™¨åŠŸèƒ½")
    print("="*80)
    
    scheduler = NewsScheduler()
    
    # æ³¨å†Œä¸€äº›æµ‹è¯•çˆ¬è™«
    print("\nğŸ“‹ æ³¨å†Œæµ‹è¯•çˆ¬è™«:")
    test_crawlers = [
        ('test_cnn', lambda: ENHANCED_CRAWLERS['cnn'](), 'CNN', 'https://www.cnn.com', 'ç¾å›½'),
        ('test_bbc', lambda: ENHANCED_CRAWLERS['bbc_chinese'](), 'BBCä¸­æ–‡', 'https://www.bbc.com/zhongwen', 'è‹±å›½'),
        ('test_zaobao', lambda: ENHANCED_CRAWLERS['zaobao'](), 'è”åˆæ—©æŠ¥', 'https://www.zaobao.com.sg', 'æ–°åŠ å¡'),
    ]
    
    for source_id, factory, name, url, country in test_crawlers:
        try:
            scheduler.register_crawler(source_id, factory, name, url, country)
            print(f"  âœ… {name} ({country})")
        except Exception as e:
            print(f"  âŒ {name}: {e}")
    
    # æµ‹è¯•è°ƒåº¦å™¨çŠ¶æ€
    print(f"\nğŸ“Š è°ƒåº¦å™¨çŠ¶æ€:")
    print(f"  - å·²æ³¨å†Œçˆ¬è™«: {len(scheduler.crawlers)}")
    print(f"  - è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if scheduler.running else 'å·²åœæ­¢'}")
    
    # åˆå§‹åŒ–ä»»åŠ¡
    print(f"\nğŸ”„ åˆå§‹åŒ–ä»»åŠ¡:")
    scheduler.init_tasks_from_config(interval_minutes=60)
    tasks = scheduler.db.get_all_tasks()
    print(f"  - å·²åˆ›å»ºä»»åŠ¡: {len(tasks)}")
    for task in tasks[:3]:
        print(f"    â€¢ {task.source_name} - é—´éš”: {task.interval_minutes}åˆ†é’Ÿ")
    
    # æµ‹è¯•ç»Ÿè®¡
    print(f"\nğŸ“ˆ ä»»åŠ¡ç»Ÿè®¡:")
    stats = scheduler.get_task_statistics()
    print(f"  - æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
    print(f"  - å¯ç”¨ä»»åŠ¡: {stats['enabled_tasks']}")
    print(f"  - åœç”¨ä»»åŠ¡: {stats['disabled_tasks']}")
    print(f"  - è¦†ç›–å›½å®¶: {stats['countries']}")

def test_crawler_capabilities():
    """æµ‹è¯•çˆ¬è™«èƒ½åŠ›"""
    print("\n" + "="*80)
    print("æµ‹è¯•3: çˆ¬è™«èƒ½åŠ›éªŒè¯")
    print("="*80)
    
    # æµ‹è¯•å¢å¼ºç‰ˆçˆ¬è™«çš„é…ç½®
    print("\nğŸ”§ é€‰æ‹©å™¨é…ç½®:")
    zaobao = ENHANCED_CRAWLERS['zaobao']()
    if hasattr(zaobao, 'selector_config') and zaobao.selector_config:
        config = zaobao.selector_config
        print(f"  âœ… è”åˆæ—©æŠ¥é€‰æ‹©å™¨:")
        print(f"    - åˆ—è¡¨å®¹å™¨: {config.list_container}")
        print(f"    - åˆ—è¡¨é¡¹: {config.list_item}")
        print(f"    - æ ‡é¢˜é€‰æ‹©å™¨: {config.list_title}")
        print(f"    - å†…å®¹é€‰æ‹©å™¨: {config.article_content}")
    
    # æµ‹è¯•åçˆ¬é…ç½®
    print("\nğŸ›¡ï¸ åçˆ¬é…ç½®:")
    if hasattr(zaobao, 'anti_crawler_config') and zaobao.anti_crawler_config:
        anti_config = zaobao.anti_crawler_config
        print(f"  âœ… åçˆ¬ç­–ç•¥:")
        print(f"    - å»¶è¿ŸèŒƒå›´: {anti_config.min_delay}-{anti_config.max_delay}ç§’")
        print(f"    - User-Agentæ•°: {len(anti_config.user_agents)}")
        print(f"    - é‡è¯•æ¬¡æ•°: {anti_config.max_retries}")
    
    # æµ‹è¯•åŸºç¡€æ–¹æ³•
    print("\nğŸ“ åŸºç¡€æ–¹æ³•:")
    try:
        article_id = zaobao.get_article_id()
        print(f"  âœ… ç”Ÿæˆæ–‡ç« ID: {article_id}")
    except Exception as e:
        print(f"  âŒ ç”Ÿæˆæ–‡ç« IDå¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ§ª NewsCrawler åŠŸèƒ½æ¼”ç¤ºæµ‹è¯•")
    print("="*80)
    
    try:
        test_crawler_instantiation()
        test_scheduler()
        test_crawler_capabilities()
        
        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºæµ‹è¯•å®Œæˆï¼")
        print("="*80)
        print("\nğŸ’¡ ç³»ç»ŸåŠŸèƒ½éªŒè¯:")
        print("  âœ… çˆ¬è™«å¯ä»¥æ­£å¸¸å®ä¾‹åŒ–")
        print("  âœ… è°ƒåº¦å™¨åŠŸèƒ½å®Œæ•´")
        print("  âœ… ä»»åŠ¡ç®¡ç†æ­£å¸¸å·¥ä½œ")
        print("  âœ… é€‰æ‹©å™¨å’Œåçˆ¬é…ç½®æ­£ç¡®")
        print("\nğŸš€ ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®é™…çˆ¬å–å·¥ä½œï¼\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
